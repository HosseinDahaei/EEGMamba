{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e442349e",
      "metadata": {
        "id": "e442349e"
      },
      "source": [
        "# EEGMamba Finetuning on PhysioNet-MI Dataset\n",
        "\n",
        "This notebook provides a complete guide for finetuning EEGMamba on the PhysioNet Motor Imagery dataset in Google Colab.\n",
        "\n",
        "## Dataset Overview\n",
        "- **Dataset**: PhysioNet Motor Movement/Imagery Dataset (EEG-MMI)\n",
        "- **Task**: Motor imagery classification (4 classes: left hand, right hand, feet, tongue)\n",
        "- **Subjects**: 109 subjects (70 train, 19 val, 20 test)\n",
        "- **Channels**: 64 EEG channels\n",
        "- **Sampling Rate**: 200 Hz after preprocessing\n",
        "- **Data Format**: LMDB database with preprocessed epochs\n",
        "\n",
        "## Prerequisites\n",
        "‚úÖ Google Drive with EEGMamba repository  \n",
        "‚úÖ Preprocessed PhysioNet data in LMDB format  \n",
        "‚úÖ GPU-enabled Colab runtime (recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5fa14c",
      "metadata": {
        "id": "6f5fa14c"
      },
      "source": [
        "## Step 1: Environment Setup and Dependencies\n",
        "\n",
        "First, let's mount Google Drive and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "r_ybs2gAJPKj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ybs2gAJPKj",
        "outputId": "2efbbaa4-a176-4ba7-9004-689ec941602b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing all critical imports...\n",
            "‚úÖ Basic packages imported successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mahmood/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Mamba-SSM ecosystem imported successfully!\n",
            "\n",
            "üìã Package versions:\n",
            "   ‚Ä¢ PyTorch: 2.5.1+cu121\n",
            "   ‚Ä¢ NumPy: 1.26.2\n",
            "   ‚Ä¢ MNE: 1.10.2\n",
            "   ‚Ä¢ Mamba-SSM: 2.2.6.post3\n",
            "\n",
            "üéâ All dependencies are working! Ready to proceed.\n"
          ]
        }
      ],
      "source": [
        "# Verify all imports work correctly\n",
        "print(\"üîç Testing all critical imports...\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    import einops\n",
        "    import numpy as np\n",
        "    import scipy\n",
        "    import sklearn\n",
        "    import mne\n",
        "    import lmdb\n",
        "    print(\"‚úÖ Basic packages imported successfully!\")\n",
        "    \n",
        "    # Test the critical ones\n",
        "    import mamba_ssm\n",
        "    import causal_conv1d\n",
        "    print(\"‚úÖ Mamba-SSM ecosystem imported successfully!\")\n",
        "    \n",
        "    # Print versions for debugging\n",
        "    print(f\"\\nüìã Package versions:\")\n",
        "    print(f\"   ‚Ä¢ PyTorch: {torch.__version__}\")\n",
        "    print(f\"   ‚Ä¢ NumPy: {np.__version__}\")\n",
        "    print(f\"   ‚Ä¢ MNE: {mne.__version__}\")\n",
        "    print(f\"   ‚Ä¢ Mamba-SSM: {mamba_ssm.__version__}\")\n",
        "    \n",
        "    print(\"\\nüéâ All dependencies are working! Ready to proceed.\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import failed: {e}\")\n",
        "    print(\"\\nüîß If you see import errors:\")\n",
        "    print(\"1. Restart runtime (Runtime ‚Üí Restart Runtime)\")\n",
        "    print(\"2. Re-run the installation cells\")\n",
        "    print(\"3. Check error messages for specific package issues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d40706b",
      "metadata": {},
      "source": [
        "## Step 2: Data Preprocessing (Optional)\n",
        "\n",
        "‚ö†Ô∏è **Note**: This step is only needed if you haven't preprocessed your data yet. If you already have the processed LMDB database, skip to Step 3.\n",
        "\n",
        "The following cell contains the preprocessing script for PhysioNet-MI data. It will:\n",
        "- Load raw EEG files from PhysioNet dataset\n",
        "- Apply filtering, rereferencing, and resampling\n",
        "- Extract motor imagery epochs\n",
        "- Save to LMDB database format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4e6efa28",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß PhysioNet-MI Preprocessing Configuration:\n",
            "üìÇ Raw data path: /home/mahmood/HosseinDahaei/Codes/EEGMamba/\n",
            "üíæ Output database: /home/mahmood/HosseinDahaei/Codes/EEGMamba/data\n",
            "üéØ Tasks: ['04', '06', '08', '10', '12', '14']\n",
            "‚úÖ Raw data directory found!\n",
            "üìä Found 9 subjects: ['.tmp', 'data', 'datasets', 'figure', 'models']...['models', 'modules', 'preprocessing', 'pretrained_weights', 'utils']\n",
            "üìà Data split: Train=9, Val=0, Test=0\n",
            "\n",
            "‚è∏Ô∏è Preprocessing skipped. Set RUN_PREPROCESSING=True to run.\n"
          ]
        }
      ],
      "source": [
        "# PhysioNet-MI Data Preprocessing Script\n",
        "# Run this only if you need to preprocess raw data\n",
        "\n",
        "import os\n",
        "import lmdb\n",
        "import pickle\n",
        "import numpy as np\n",
        "import mne\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "tasks = ['04', '06', '08', '10', '12', '14']  # Motor imagery tasks\n",
        "root_dir = '/home/mahmood/HosseinDahaei/Codes/EEGMamba/'\n",
        "output_db_path = '/home/mahmood/HosseinDahaei/Codes/EEGMamba/data'\n",
        "\n",
        "print(\"üîß PhysioNet-MI Preprocessing Configuration:\")\n",
        "print(f\"üìÇ Raw data path: {root_dir}\")\n",
        "print(f\"üíæ Output database: {output_db_path}\")\n",
        "print(f\"üéØ Tasks: {tasks}\")\n",
        "\n",
        "# Check if raw data exists\n",
        "if not os.path.exists(root_dir):\n",
        "    print(\"‚ùå Raw data directory not found!\")\n",
        "    print(\"Please ensure PhysioNet data is downloaded to the specified path.\")\n",
        "else:\n",
        "    print(\"‚úÖ Raw data directory found!\")\n",
        "    \n",
        "    # List available subjects\n",
        "    files = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
        "    files = sorted(files)\n",
        "    print(f\"üìä Found {len(files)} subjects: {files[:5]}...{files[-5:]}\")\n",
        "    \n",
        "    # Split subjects\n",
        "    files_dict = {\n",
        "        'train': files[:70],\n",
        "        'val': files[70:89], \n",
        "        'test': files[89:109],\n",
        "    }\n",
        "    \n",
        "    print(f\"üìà Data split: Train={len(files_dict['train'])}, Val={len(files_dict['val'])}, Test={len(files_dict['test'])}\")\n",
        "\n",
        "# Set this to True only if you want to run preprocessing\n",
        "RUN_PREPROCESSING = False\n",
        "\n",
        "if RUN_PREPROCESSING:\n",
        "    print(\"\\nüöÄ Starting preprocessing...\")\n",
        "else:\n",
        "    print(\"\\n‚è∏Ô∏è Preprocessing skipped. Set RUN_PREPROCESSING=True to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d4e766d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∏Ô∏è Preprocessing skipped\n"
          ]
        }
      ],
      "source": [
        "# Actual preprocessing implementation (only runs if enabled above)\n",
        "if RUN_PREPROCESSING and os.path.exists(root_dir):\n",
        "    print(\"üîÑ Running preprocessing...\")\n",
        "    \n",
        "    # EEG channel selection (64 channels)\n",
        "    selected_channels = [\n",
        "        'Fc5.', 'Fc3.', 'Fc1.', 'Fcz.', 'Fc2.', 'Fc4.', 'Fc6.', \n",
        "        'C5..', 'C3..', 'C1..', 'Cz..', 'C2..', 'C4..', 'C6..', \n",
        "        'Cp5.', 'Cp3.', 'Cp1.', 'Cpz.', 'Cp2.', 'Cp4.', 'Cp6.', \n",
        "        'Fp1.', 'Fpz.', 'Fp2.', 'Af7.', 'Af3.', 'Afz.', 'Af4.', 'Af8.', \n",
        "        'F7..', 'F5..', 'F3..', 'F1..', 'Fz..', 'F2..', 'F4..', 'F6..', 'F8..', \n",
        "        'Ft7.', 'Ft8.', 'T7..', 'T8..', 'T9..', 'T10.', 'Tp7.', 'Tp8.', \n",
        "        'P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', \n",
        "        'Po7.', 'Po3.', 'Poz.', 'Po4.', 'Po8.', 'O1..', 'Oz..', 'O2..', 'Iz..'\n",
        "    ]\n",
        "    \n",
        "    # Initialize LMDB database\n",
        "    db = lmdb.open(output_db_path, map_size=4614542346)\n",
        "    dataset = {'train': [], 'val': [], 'test': []}\n",
        "    \n",
        "    # Process each split\n",
        "    for split_name, file_list in files_dict.items():\n",
        "        print(f\"\\nüìä Processing {split_name} set ({len(file_list)} subjects)...\")\n",
        "        \n",
        "        for file in tqdm(file_list, desc=f\"{split_name}\"):\n",
        "            for task in tasks:\n",
        "                try:\n",
        "                    # Load EEG file\n",
        "                    file_path = os.path.join(root_dir, file, f'{file}R{task}.edf')\n",
        "                    raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
        "                    \n",
        "                    # Preprocessing pipeline\n",
        "                    raw.pick_channels(selected_channels, ordered=True)\n",
        "                    if len(raw.info['bads']) > 0:\n",
        "                        raw.interpolate_bads()\n",
        "                    raw.set_eeg_reference(ref_channels='average')\n",
        "                    raw.filter(l_freq=0.3, h_freq=None, verbose=False)\n",
        "                    raw.notch_filter(60, verbose=False)\n",
        "                    raw.resample(200, verbose=False)\n",
        "                    \n",
        "                    # Extract epochs\n",
        "                    events_from_annot, event_dict = mne.events_from_annotations(raw, verbose=False)\n",
        "                    epochs = mne.Epochs(raw, events_from_annot, event_dict, \n",
        "                                      tmin=0, tmax=4-1.0/raw.info['sfreq'], \n",
        "                                      baseline=None, preload=True, verbose=False)\n",
        "                    \n",
        "                    # Get data and reshape\n",
        "                    data = epochs.get_data(units='uV')[:, :, -800:]  # Last 4 seconds at 200Hz\n",
        "                    events = epochs.events[:, 2]\n",
        "                    \n",
        "                    # Reshape to (batch, channels, time_segments, samples_per_segment)\n",
        "                    bz, ch_nums, _ = data.shape\n",
        "                    data = data.reshape(bz, ch_nums, 4, 200)\n",
        "                    \n",
        "                    # Save to LMDB\n",
        "                    for i, (sample, event) in enumerate(zip(data, events)):\n",
        "                        if event != 1:  # Skip rest events\n",
        "                            sample_key = f'{file}R{task}-{i}'\n",
        "                            data_dict = {\n",
        "                                'sample': sample,\n",
        "                                'label': event - 2 if task in ['04', '08', '12'] else event\n",
        "                            }\n",
        "                            txn = db.begin(write=True)\n",
        "                            txn.put(key=sample_key.encode(), value=pickle.dumps(data_dict))\n",
        "                            txn.commit()\n",
        "                            dataset[split_name].append(sample_key)\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error processing {file}R{task}: {str(e)[:100]}...\")\n",
        "                    continue\n",
        "    \n",
        "    # Save dataset keys\n",
        "    txn = db.begin(write=True)\n",
        "    txn.put(key='__keys__'.encode(), value=pickle.dumps(dataset))\n",
        "    txn.commit()\n",
        "    db.close()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "    print(f\"üìä Dataset saved with {sum(len(v) for v in dataset.values())} total samples\")\n",
        "    for split, samples in dataset.items():\n",
        "        print(f\"   {split}: {len(samples)} samples\")\n",
        "        \n",
        "else:\n",
        "    if not RUN_PREPROCESSING:\n",
        "        print(\"‚è∏Ô∏è Preprocessing skipped\")\n",
        "    else:\n",
        "        print(\"‚ùå Cannot run preprocessing - raw data path not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "af1a767d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af1a767d",
        "outputId": "22f774c9-e5f3-44b6-aed9-af42f7156b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking for processed data at: /home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average\n",
            "‚úÖ Processed data directory found!\n",
            "üìÅ Contents: ['lock.mdb', 'data.mdb']\n",
            "‚úÖ LMDB database files present - data is ready for training!\n",
            "üìä Dataset splits:\n",
            "   train: 6300 samples\n",
            "   val: 1734 samples\n",
            "   test: 1758 samples\n",
            "üìà Total samples: 9792\n"
          ]
        }
      ],
      "source": [
        "# Verify preprocessed data exists\n",
        "import os\n",
        "\n",
        "# Check for processed data in Google Drive\n",
        "data_path = \"/home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average\"\n",
        "print(f\"üîç Checking for processed data at: {data_path}\")\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    files = os.listdir(data_path)\n",
        "    print(f\"‚úÖ Processed data directory found!\")\n",
        "    print(f\"üìÅ Contents: {files}\")\n",
        "    \n",
        "    # Check for LMDB database files\n",
        "    if 'data.mdb' in files and 'lock.mdb' in files:\n",
        "        print(\"‚úÖ LMDB database files present - data is ready for training!\")\n",
        "        \n",
        "        # Get database info\n",
        "        import lmdb\n",
        "        db = lmdb.open(data_path, readonly=True)\n",
        "        with db.begin() as txn:\n",
        "            try:\n",
        "                keys_data = txn.get('__keys__'.encode())\n",
        "                if keys_data:\n",
        "                    dataset_keys = pickle.loads(keys_data)\n",
        "                    print(f\"üìä Dataset splits:\")\n",
        "                    for split, samples in dataset_keys.items():\n",
        "                        print(f\"   {split}: {len(samples)} samples\")\n",
        "                    total_samples = sum(len(v) for v in dataset_keys.values())\n",
        "                    print(f\"üìà Total samples: {total_samples}\")\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è Dataset keys not found in database\")\n",
        "            except:\n",
        "                print(\"‚ö†Ô∏è Could not read dataset keys\")\n",
        "        db.close()\n",
        "    else:\n",
        "        print(\"‚ùå LMDB database files missing - preprocessing needed\")\n",
        "        print(\"üí° Set RUN_PREPROCESSING=True in the previous cell to create the database\")\n",
        "else:\n",
        "    print(\"‚ùå Processed data directory not found\")\n",
        "    print(\"üí° Options:\")\n",
        "    print(\"   1. Set RUN_PREPROCESSING=True above to preprocess data\")\n",
        "    print(\"   2. Check if the path is correct\")\n",
        "    print(\"   3. Upload preprocessed data to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbbfe336",
      "metadata": {
        "id": "cbbfe336"
      },
      "source": [
        "## Step 4: Test Dataset Loading\n",
        "\n",
        "Let's test the EEGMamba dataset loader to understand our data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a4e40839",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4e40839",
        "outputId": "2ce5259b-d5c8-49f1-863c-8dd2e42f4056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing EEGMamba dataset loader...\n",
            "üìÇ Loading dataset from: /home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average\n",
            "6300 1734 1758\n",
            "9792\n",
            "‚úÖ Dataset loaded successfully!\n",
            "üìä Available data splits: ['train', 'val', 'test']\n",
            "\n",
            "üîç Examining TRAIN set:\n",
            "   Number of batches: 788\n",
            "   ‚úÖ Batch 1 loaded successfully\n",
            "   üìä Data shape: torch.Size([8, 64, 4, 200])\n",
            "   üéØ Labels shape: torch.Size([8])\n",
            "   üìà Data type: torch.float32\n",
            "   üìâ Data range: [-3.316, 5.211]\n",
            "   üè∑Ô∏è Unique labels: [0, 1, 3]\n",
            "   üìù Label meanings: 0=left_hand, 1=right_hand, 2=feet, 3=tongue\n",
            "\n",
            "üéâ Dataset loading test completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Test the EEGMamba dataset loader\n",
        "print(\"üß™ Testing EEGMamba dataset loader...\")\n",
        "\n",
        "try:\n",
        "    from datasets.physio_dataset import LoadDataset\n",
        "    \n",
        "    # Create test parameters for dataset loading\n",
        "    class TestParams:\n",
        "        def __init__(self):\n",
        "            self.datasets_dir = \"/home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average\"\n",
        "            self.batch_size = 8\n",
        "    \n",
        "    # Initialize dataset loader\n",
        "    test_params = TestParams()\n",
        "    print(f\"üìÇ Loading dataset from: {test_params.datasets_dir}\")\n",
        "    \n",
        "    dataset_loader = LoadDataset(test_params)\n",
        "    data_loaders = dataset_loader.get_data_loader()\n",
        "    \n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"üìä Available data splits: {list(data_loaders.keys())}\")\n",
        "    \n",
        "    # Test data loading\n",
        "    for split_name, data_loader in data_loaders.items():\n",
        "        print(f\"\\nüîç Examining {split_name.upper()} set:\")\n",
        "        print(f\"   Number of batches: {len(data_loader)}\")\n",
        "        \n",
        "        # Get first batch to examine data structure\n",
        "        for batch_idx, (data, labels) in enumerate(data_loader):\n",
        "            print(f\"   ‚úÖ Batch {batch_idx + 1} loaded successfully\")\n",
        "            print(f\"   üìä Data shape: {data.shape}\")\n",
        "            print(f\"   üéØ Labels shape: {labels.shape}\")\n",
        "            print(f\"   üìà Data type: {data.dtype}\")\n",
        "            print(f\"   üìâ Data range: [{data.min():.3f}, {data.max():.3f}]\")\n",
        "            \n",
        "            # Check labels\n",
        "            unique_labels = torch.unique(labels).tolist()\n",
        "            print(f\"   üè∑Ô∏è Unique labels: {unique_labels}\")\n",
        "            print(f\"   üìù Label meanings: 0=left_hand, 1=right_hand, 2=feet, 3=tongue\")\n",
        "            break  # Only examine first batch\n",
        "        break  # Only examine first split for testing\n",
        "        \n",
        "    print(\"\\nüéâ Dataset loading test completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing dataset loader: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"1. Make sure preprocessed data exists\")\n",
        "    print(\"2. Check the datasets_dir path\")\n",
        "    print(\"3. Verify LMDB database integrity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "990e8168",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "990e8168",
        "outputId": "b992bb09-52c8-4640-cad0-30998211cb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Detailed Data Analysis\n",
            "\n",
            "--- TRAIN SET ANALYSIS ---\n",
            "üìê Data tensor shape: torch.Size([8, 64, 4, 200])\n",
            "   ‚Ä¢ Batch size: 8\n",
            "   ‚Ä¢ Channels: 64\n",
            "   ‚Ä¢ Time segments: 4\n",
            "   ‚Ä¢ Samples per segment: 200\n",
            "   ‚Ä¢ Total time points: 800\n",
            "   ‚Ä¢ Time duration: 4.0 seconds (at 200 Hz)\n",
            "\n",
            "üè∑Ô∏è Label Distribution (first 6 batches):\n",
            "   Left Hand (class 0): 13 samples\n",
            "   Right Hand (class 1): 9 samples\n",
            "   Feet (class 2): 16 samples\n",
            "   Tongue (class 3): 10 samples\n",
            "üìä Total samples analyzed: 48\n",
            "\n",
            "‚úÖ Data analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# Detailed data analysis\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"üìä Detailed Data Analysis\")\n",
        "\n",
        "if 'data_loaders' in locals():\n",
        "    for split_name, data_loader in data_loaders.items():\n",
        "        print(f\"\\n--- {split_name.upper()} SET ANALYSIS ---\")\n",
        "        \n",
        "        all_labels = []\n",
        "        batch_count = 0\n",
        "        sample_count = 0\n",
        "        \n",
        "        # Analyze several batches\n",
        "        for batch_idx, (data, labels) in enumerate(data_loader):\n",
        "            batch_count += 1\n",
        "            sample_count += data.shape[0]\n",
        "            all_labels.extend(labels.tolist())\n",
        "            \n",
        "            if batch_idx == 0:  # Detailed analysis of first batch\n",
        "                print(f\"üìê Data tensor shape: {data.shape}\")\n",
        "                print(f\"   ‚Ä¢ Batch size: {data.shape[0]}\")\n",
        "                print(f\"   ‚Ä¢ Channels: {data.shape[1]}\")\n",
        "                print(f\"   ‚Ä¢ Time segments: {data.shape[2]}\")\n",
        "                print(f\"   ‚Ä¢ Samples per segment: {data.shape[3]}\")\n",
        "                print(f\"   ‚Ä¢ Total time points: {data.shape[2] * data.shape[3]}\")\n",
        "                print(f\"   ‚Ä¢ Time duration: {data.shape[2] * data.shape[3] / 200:.1f} seconds (at 200 Hz)\")\n",
        "                \n",
        "            # Don't load all data to save memory\n",
        "            if batch_idx >= 5:  # Analyze first 5 batches only\n",
        "                break\n",
        "                \n",
        "        # Label distribution\n",
        "        unique_labels, counts = torch.unique(torch.tensor(all_labels), return_counts=True)\n",
        "        print(f\"\\nüè∑Ô∏è Label Distribution (first {batch_count} batches):\")\n",
        "        label_names = ['Left Hand', 'Right Hand', 'Feet', 'Tongue']\n",
        "        for label, count in zip(unique_labels.tolist(), counts.tolist()):\n",
        "            if label < len(label_names):\n",
        "                print(f\"   {label_names[label]} (class {label}): {count} samples\")\n",
        "        \n",
        "        print(f\"üìä Total samples analyzed: {len(all_labels)}\")\n",
        "        \n",
        "        # Only analyze first split to save time\n",
        "        break\n",
        "        \n",
        "    print(\"\\n‚úÖ Data analysis completed!\")\n",
        "else:\n",
        "    print(\"‚ùå No data loaders available. Please run the previous cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0023e291",
      "metadata": {
        "id": "0023e291"
      },
      "source": [
        "## Step 5: Training Configuration\n",
        "\n",
        "Now let's set up the training parameters for EEGMamba finetuning on PhysioNet-MI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b275358",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b275358",
        "outputId": "9edc1192-51ae-4d5e-b13b-9fa053e273d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è EEGMamba Training Configuration\n",
            "üìã Configuration Summary:\n",
            "   üéØ Dataset: PhysioNet-MI\n",
            "   üìÇ Data path: /home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average\n",
            "   üè∑Ô∏è Classes: 4\n",
            "   üèÉ Epochs: 5\n",
            "   üì¶ Batch size: 16\n",
            "   üìà Learning rate: 0.0001\n",
            "   üíæ Model save dir: ./results/physio_models\n",
            "   üé≠ Use pretrained: True\n",
            "   ‚úÖ Pretrained weights found: pretrained_weights/pretrained_weights.pth\n",
            "   üìÅ Results directory ready: ./results/physio_models\n"
          ]
        }
      ],
      "source": [
        "# EEGMamba Training Configuration\n",
        "print(\"‚öôÔ∏è EEGMamba Training Configuration\")\n",
        "\n",
        "# Dataset and paths\n",
        "DATASET_NAME = \"PhysioNet-MI\"\n",
        "DATASETS_DIR = \"/home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average\"\n",
        "MODEL_DIR = \"./results/physio_models\"\n",
        "PRETRAINED_WEIGHTS = \"pretrained_weights/pretrained_weights.pth\"\n",
        "\n",
        "# Model parameters\n",
        "NUM_CLASSES = 4  # Motor imagery classes: left hand, right hand, feet, tongue\n",
        "CLASSIFIER_TYPE = \"all_patch_reps\"  # EEGMamba classifier type\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 5  # Start with quick test, increase later\n",
        "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 5e-2\n",
        "CUDA_DEVICE = 0\n",
        "\n",
        "# Advanced options\n",
        "OPTIMIZER = \"AdamW\"\n",
        "DROPOUT = 0.1\n",
        "LABEL_SMOOTHING = 0.1\n",
        "USE_PRETRAINED = True\n",
        "\n",
        "print(\"üìã Configuration Summary:\")\n",
        "print(f\"   üéØ Dataset: {DATASET_NAME}\")\n",
        "print(f\"   üìÇ Data path: {DATASETS_DIR}\")\n",
        "print(f\"   üè∑Ô∏è Classes: {NUM_CLASSES}\")\n",
        "print(f\"   üèÉ Epochs: {EPOCHS}\")\n",
        "print(f\"   üì¶ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   üìà Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   üíæ Model save dir: {MODEL_DIR}\")\n",
        "print(f\"   üé≠ Use pretrained: {USE_PRETRAINED}\")\n",
        "\n",
        "# Verify pretrained weights exist\n",
        "if USE_PRETRAINED:\n",
        "    if os.path.exists(PRETRAINED_WEIGHTS):\n",
        "        print(f\"   ‚úÖ Pretrained weights found: {PRETRAINED_WEIGHTS}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Pretrained weights not found: {PRETRAINED_WEIGHTS}\")\n",
        "        print(\"   üí° Will train from scratch or download weights\")\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "print(f\"   üìÅ Results directory ready: {MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "CJXh-FCpM0fv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJXh-FCpM0fv",
        "outputId": "5f9015d6-76a3-4623-bc34-81e29ea6fe12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Pre-training Environment Check\n",
            "‚úÖ CUDA available: NVIDIA RTX A4000\n",
            "   GPU memory: 16.8 GB\n",
            "‚úÖ Main training script: finetune_main.py\n",
            "‚úÖ Model definitions: models/\n",
            "‚úÖ Dataset loaders: datasets/\n",
            "‚úÖ Pretrained weights: pretrained_weights/\n",
            "‚úÖ PhysioNet model import successful\n",
            "\n",
            "üöÄ Ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Quick environment check before training\n",
        "print(\"üîç Pre-training Environment Check\")\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CUDA not available - training will be slow on CPU\")\n",
        "\n",
        "# Check key files\n",
        "key_files = {\n",
        "    'finetune_main.py': 'Main training script',\n",
        "    'models/': 'Model definitions',\n",
        "    'datasets/': 'Dataset loaders',\n",
        "    'pretrained_weights/': 'Pretrained weights'\n",
        "}\n",
        "\n",
        "for file_path, description in key_files.items():\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"‚úÖ {description}: {file_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Missing {description}: {file_path}\")\n",
        "\n",
        "# Check if we can import the model\n",
        "try:\n",
        "    from models.model_for_physio import Model\n",
        "    print(\"‚úÖ PhysioNet model import successful\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model import failed: {e}\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b953fb",
      "metadata": {
        "id": "e5b953fb"
      },
      "source": [
        "## Step 4: Quick Test Run (5 epochs)\n",
        "\n",
        "Let's first do a quick test with fewer epochs to make sure everything works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d16516b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d16516b0",
        "outputId": "401b7ea1-a3ee-433b-ea62-cd3afa9307dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Command:\n",
            "python finetune_main.py     --downstream_dataset PhysioNet-MI     --datasets_dir /home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average     --num_of_classes 4     --model_dir ./results/physio_models_test     --epochs 5     --batch_size 16     --lr 0.0001     --weight_decay 0.05     --cuda 0     --use_pretrained_weights True     --foundation_dir pretrained_weights/pretrained_weights.pth\n",
            "\n",
            "üìù Copy and run this command in terminal to test the setup!\n"
          ]
        }
      ],
      "source": [
        "# Quick test command\n",
        "test_command = f\"\"\"\n",
        "python finetune_main.py \\\n",
        "    --downstream_dataset {DATASET_NAME} \\\n",
        "    --datasets_dir {DATASETS_DIR} \\\n",
        "    --num_of_classes {NUM_CLASSES} \\\n",
        "    --model_dir {MODEL_DIR}_test \\\n",
        "    --epochs 5 \\\n",
        "    --batch_size 16 \\\n",
        "    --lr {LEARNING_RATE} \\\n",
        "    --weight_decay {WEIGHT_DECAY} \\\n",
        "    --cuda {CUDA_DEVICE} \\\n",
        "    --use_pretrained_weights True \\\n",
        "    --foundation_dir {PRETRAINED_WEIGHTS}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Quick Test Command:\")\n",
        "print(test_command.strip())\n",
        "print(\"\\nüìù Copy and run this command in terminal to test the setup!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c550961",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8282710",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Quick Test Run (5 epochs)...\n",
            "This will test the setup with a short training run.\n",
            "üìã Command: /opt/conda/envs/eegmamba3/bin/python finetune_main.py --downstream_dataset PhysioNet-MI --datasets_dir /home/mahmood/HosseinDahaei/Codes/EEGMamba/data/raw_motor_movement_Imagery/processed_average --num_of_classes 4 --model_dir ./results/physio_models_test --epochs 1 --batch_size 16 --lr 0.0001 --weight_decay 0.05 --cuda 0 --use_pretrained_weights True --foundation_dir pretrained_weights/pretrained_weights.pth\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Execute quick test run (5 epochs)\n",
        "print(\"üöÄ Starting Quick Test Run (5 epochs)...\")\n",
        "print(\"This will test the setup with a short training run.\")\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Build the test command\n",
        "test_args = [\n",
        "    sys.executable, \"finetune_main.py\",\n",
        "    \"--downstream_dataset\", DATASET_NAME,\n",
        "    \"--datasets_dir\", DATASETS_DIR,\n",
        "    \"--num_of_classes\", str(NUM_CLASSES),\n",
        "    \"--model_dir\", f\"{MODEL_DIR}_test\",\n",
        "    \"--epochs\", \"1\",\n",
        "    \"--batch_size\", \"16\", \n",
        "    \"--lr\", str(LEARNING_RATE),\n",
        "    \"--weight_decay\", str(WEIGHT_DECAY),\n",
        "    \"--cuda\", str(CUDA_DEVICE),\n",
        "    \"--use_pretrained_weights\", \"True\",\n",
        "    \"--foundation_dir\", PRETRAINED_WEIGHTS\n",
        "]\n",
        "\n",
        "print(f\"üìã Command: {' '.join(test_args)}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Execute the command\n",
        "try:\n",
        "    result = subprocess.run(test_args, capture_output=True, text=True, timeout=3600)  # 60 min timeout\n",
        "    \n",
        "    print(\"STDOUT:\")\n",
        "    print(result.stdout)\n",
        "    \n",
        "    if result.stderr:\n",
        "        print(\"\\nSTDERR:\")\n",
        "        print(result.stderr)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n‚úÖ Test run completed successfully!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Test run failed with return code: {result.returncode}\")\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚è±Ô∏è Command timed out after 30 minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error running command: {e}\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ef454a",
      "metadata": {},
      "source": [
        "### üöÄ Execute Test Run\n",
        "\n",
        "**Run this cell to execute the quick test training (5 epochs):**\n",
        "- Tests that everything is set up correctly\n",
        "- Takes ~10-15 minutes\n",
        "- Uses smaller batch size for memory safety\n",
        "- Results saved to `{MODEL_DIR}_test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MqydFuhhNTNJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqydFuhhNTNJ",
        "outputId": "06cd3b40-e764-4386-87be-88565f93af5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up EEGMamba environment...\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (1.7.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.3.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.6)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.3.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "üì¶ Installing causal-conv1d...\n",
            "Collecting causal-conv1d\n",
            "  Downloading causal_conv1d-1.5.2.tar.gz (23 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from causal-conv1d) (2.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from causal-conv1d) (25.0)\n",
            "Collecting ninja (from causal-conv1d)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->causal-conv1d) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->causal-conv1d) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->causal-conv1d) (3.0.3)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal-conv1d\n",
            "  Building wheel for causal-conv1d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal-conv1d: filename=causal_conv1d-1.5.2-cp312-cp312-linux_x86_64.whl size=151160839 sha256=7cc4ae241543f93acfa4d96ecbc43e532f3aeed171f88c859f873f1a617e9606\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b165gau6/wheels/b4/a5/a7/8d0ecdd7a890f633986e0e42bd99700dc256cbf33b67057f9f\n",
            "Successfully built causal-conv1d\n",
            "Installing collected packages: ninja, causal-conv1d\n",
            "Successfully installed causal-conv1d-1.5.2 ninja-1.13.0\n",
            "üì¶ Installing mamba-ssm (this may take a few minutes)...\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.5.tar.gz (113 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies for EEGMamba (optimized for Colab)\n",
        "print(\"üîß Setting up EEGMamba environment...\")\n",
        "\n",
        "# First, install basic dependencies\n",
        "!pip install einops lmdb torch torchvision torchaudio\n",
        "!pip install scipy scikit-learn matplotlib\n",
        "\n",
        "# Install causal-conv1d first (required for mamba-ssm)\n",
        "print(\"üì¶ Installing causal-conv1d...\")\n",
        "!pip install causal-conv1d --no-cache-dir\n",
        "\n",
        "# Install mamba-ssm with specific flags for faster compilation\n",
        "print(\"üì¶ Installing mamba-ssm (this may take a few minutes)...\")\n",
        "!pip install mamba-ssm --no-cache-dir\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n",
        "print(\"üîÑ Please restart runtime if running in Colab for changes to take effect.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edd9b74",
      "metadata": {
        "id": "0edd9b74"
      },
      "source": [
        "## Step 5: Full Training Command\n",
        "\n",
        "Once the test works, use this command for full training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13c321b",
      "metadata": {
        "id": "a13c321b"
      },
      "outputs": [],
      "source": [
        "# Full training command\n",
        "full_command = f\"\"\"\n",
        "python finetune_main.py \\\n",
        "    --downstream_dataset {DATASET_NAME} \\\n",
        "    --datasets_dir {DATASETS_DIR} \\\n",
        "    --num_of_classes {NUM_CLASSES} \\\n",
        "    --model_dir {MODEL_DIR} \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --batch_size {BATCH_SIZE} \\\n",
        "    --lr {LEARNING_RATE} \\\n",
        "    --weight_decay {WEIGHT_DECAY} \\\n",
        "    --cuda {CUDA_DEVICE} \\\n",
        "    --use_pretrained_weights True \\\n",
        "    --foundation_dir {PRETRAINED_WEIGHTS} \\\n",
        "    --optimizer AdamW \\\n",
        "    --classifier all_patch_reps \\\n",
        "    --dropout 0.1 \\\n",
        "    --label_smoothing 0.1\n",
        "\"\"\"\n",
        "\n",
        "print(\"Full Training Command:\")\n",
        "print(full_command.strip())\n",
        "print(\"\\nüöÄ This will run the complete training (note: it runs 100 iterations as per the code)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4cd609d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute full training run\n",
        "print(\"üöÄ Starting Full Training Run...\")\n",
        "print(\"‚ö†Ô∏è WARNING: This will run the complete training (note: default runs 100 iterations)\")\n",
        "print(\"üí° Consider modifying finetune_main.py to reduce iterations if needed\")\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Build the full training command  \n",
        "full_args = [\n",
        "    sys.executable, \"finetune_main.py\",\n",
        "    \"--downstream_dataset\", DATASET_NAME,\n",
        "    \"--datasets_dir\", DATASETS_DIR,\n",
        "    \"--num_of_classes\", str(NUM_CLASSES),\n",
        "    \"--model_dir\", MODEL_DIR,\n",
        "    \"--epochs\", str(EPOCHS),\n",
        "    \"--batch_size\", str(BATCH_SIZE),\n",
        "    \"--lr\", str(LEARNING_RATE),\n",
        "    \"--weight_decay\", str(WEIGHT_DECAY),\n",
        "    \"--cuda\", str(CUDA_DEVICE),\n",
        "    \"--use_pretrained_weights\", \"True\",\n",
        "    \"--foundation_dir\", PRETRAINED_WEIGHTS,\n",
        "    \"--optimizer\", \"AdamW\",\n",
        "    \"--classifier\", \"all_patch_reps\",\n",
        "    \"--dropout\", \"0.1\",\n",
        "    \"--label_smoothing\", \"0.1\"\n",
        "]\n",
        "\n",
        "print(f\"üìã Command: {' '.join(full_args)}\")\n",
        "\n",
        "# Ask for confirmation\n",
        "confirm = input(\"\\n‚ö†Ô∏è This is a long training run. Continue? (y/N): \")\n",
        "if confirm.lower() != 'y':\n",
        "    print(\"‚ùå Training cancelled by user\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Execute the command\n",
        "    try:\n",
        "        # Use a longer timeout for full training (6 hours)\n",
        "        result = subprocess.run(full_args, capture_output=False, text=True, timeout=21600)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        \n",
        "        print(f\"\\n‚è±Ô∏è Training completed in {duration/3600:.2f} hours\")\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Full training completed successfully!\")\n",
        "            print(f\"üìÅ Check results in: {MODEL_DIR}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Training failed with return code: {result.returncode}\")\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚è±Ô∏è Training timed out after 6 hours\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"üõë Training interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error running training: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a7ff85",
      "metadata": {},
      "source": [
        "### üèÅ Execute Full Training\n",
        "\n",
        "**Run this cell for complete training:**\n",
        "- Full hyperparameter optimization\n",
        "- Runs 100 iterations by default (as per original code)\n",
        "- Takes several hours to complete\n",
        "- Includes confirmation prompt before starting\n",
        "- Results saved to `{MODEL_DIR}`\n",
        "\n",
        "‚ö†Ô∏è **Important**: The original code runs 100 training iterations. Consider modifying `finetune_main.py` line `for i in range(1, 100):` to `range(1, 2)` if you only want one training run."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edcc9642",
      "metadata": {
        "id": "edcc9642"
      },
      "source": [
        "## Step 6: Alternative - Single Run Training\n",
        "\n",
        "If you want to run just one training iteration instead of 100, you can modify the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f670080",
      "metadata": {
        "id": "4f670080"
      },
      "outputs": [],
      "source": [
        "# Show the current PhysioNet-MI loop in finetune_main.py\n",
        "print(\"Current PhysioNet-MI section in finetune_main.py:\")\n",
        "print(\"\"\"\n",
        "elif params.downstream_dataset == 'PhysioNet-MI':\n",
        "    for i in range(1, 100):  # <-- This runs 100 times!\n",
        "        print('The {}th fold'.format(i))\n",
        "        load_dataset = physio_dataset.LoadDataset(params)\n",
        "        data_loader = load_dataset.get_data_loader()\n",
        "        model = model_for_physio.Model(params)\n",
        "        t = Trainer(params, data_loader, model)\n",
        "        t.train_for_multiclass()\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüí° To run just once, you can:\")\n",
        "print(\"1. Change 'range(1, 100)' to 'range(1, 2)' in finetune_main.py\")\n",
        "print(\"2. Or comment out the for loop entirely\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8788619f",
      "metadata": {
        "id": "8788619f"
      },
      "source": [
        "## Step 7: Monitor Training Progress\n",
        "\n",
        "During training, you can monitor progress with these commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef46e56",
      "metadata": {
        "id": "5ef46e56"
      },
      "outputs": [],
      "source": [
        "# Commands to monitor training\n",
        "print(\"Monitoring Commands:\")\n",
        "print(\"1. Watch GPU usage:\")\n",
        "print(\"   watch -n 1 nvidia-smi\")\n",
        "print(\"\\n2. Monitor log files (if any):\")\n",
        "print(\"   tail -f training.log\")\n",
        "print(\"\\n3. Check model directory:\")\n",
        "print(f\"   ls -la {MODEL_DIR}/\")\n",
        "print(\"\\n4. Monitor CPU/Memory:\")\n",
        "print(\"   htop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8380163d",
      "metadata": {
        "id": "8380163d"
      },
      "source": [
        "## Step 8: Expected Training Time and Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15407140",
      "metadata": {
        "id": "15407140"
      },
      "outputs": [],
      "source": [
        "# Estimate resources and time\n",
        "print(\"Training Resource Requirements:\")\n",
        "print(\"üíæ GPU Memory: ~8-12 GB (depending on batch size)\")\n",
        "print(\"‚è±Ô∏è  Time per epoch: ~2-5 minutes (depending on GPU)\")\n",
        "print(f\"üïê Total time estimate: ~{EPOCHS * 3} minutes for {EPOCHS} epochs\")\n",
        "print(\"üîÑ Total runs: 100 iterations (as per original code)\")\n",
        "print(f\"üìä Total estimated time: ~{EPOCHS * 3 * 100 / 60:.1f} hours for all 100 runs\")\n",
        "\n",
        "print(\"\\nüìà Dataset Size:\")\n",
        "print(\"‚Ä¢ Training samples: ~1400-1600\")\n",
        "print(\"‚Ä¢ Validation samples: ~380-420\")\n",
        "print(\"‚Ä¢ Test samples: ~400-450\")\n",
        "print(\"‚Ä¢ Total: ~2200-2500 samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce0551d",
      "metadata": {
        "id": "9ce0551d"
      },
      "source": [
        "## Step 9: Troubleshooting Common Issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92f1d02",
      "metadata": {
        "id": "b92f1d02"
      },
      "outputs": [],
      "source": [
        "print(\"Common Issues and Solutions:\")\n",
        "print(\"\\n1. CUDA Out of Memory:\")\n",
        "print(\"   ‚Üí Reduce batch_size from 32 to 16 or 8\")\n",
        "print(\"   ‚Üí Use: --batch_size 16\")\n",
        "\n",
        "print(\"\\n2. Dataset not found:\")\n",
        "print(\"   ‚Üí Check if LMDB files exist in processed_average directory\")\n",
        "print(\"   ‚Üí Run preprocessing if needed\")\n",
        "\n",
        "print(\"\\n3. Pretrained weights not found:\")\n",
        "print(\"   ‚Üí Check if pretrained_weights.pth exists\")\n",
        "print(\"   ‚Üí Use: --use_pretrained_weights False (to train from scratch)\")\n",
        "\n",
        "print(\"\\n4. Too slow training:\")\n",
        "print(\"   ‚Üí Increase batch_size if GPU memory allows\")\n",
        "print(\"   ‚Üí Reduce num_workers if CPU limited\")\n",
        "print(\"   ‚Üí Use mixed precision training (if implemented)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb9e1af",
      "metadata": {
        "id": "5cb9e1af"
      },
      "source": [
        "## Step 10: Next Steps After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd15537f",
      "metadata": {
        "id": "fd15537f"
      },
      "outputs": [],
      "source": [
        "print(\"After Training Completes:\")\n",
        "print(\"\\n1. üìä Analyze Results:\")\n",
        "print(\"   ‚Üí Check accuracy/loss curves\")\n",
        "print(\"   ‚Üí Compare with baseline results\")\n",
        "print(\"   ‚Üí Look at confusion matrices\")\n",
        "\n",
        "print(\"\\n2. üíæ Save Important Files:\")\n",
        "print(\"   ‚Üí Best model checkpoints\")\n",
        "print(\"   ‚Üí Training logs\")\n",
        "print(\"   ‚Üí Configuration files\")\n",
        "\n",
        "print(\"\\n3. üî¨ Further Experiments:\")\n",
        "print(\"   ‚Üí Try different learning rates\")\n",
        "print(\"   ‚Üí Experiment with classifier types\")\n",
        "print(\"   ‚Üí Adjust data augmentation\")\n",
        "print(\"   ‚Üí Compare with other baselines\")\n",
        "\n",
        "print(\"\\n4. üìù Document Results:\")\n",
        "print(\"   ‚Üí Record best accuracy achieved\")\n",
        "print(\"   ‚Üí Note optimal hyperparameters\")\n",
        "print(\"   ‚Üí Save example predictions\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "eegmamba3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
